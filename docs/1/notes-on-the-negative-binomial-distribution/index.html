<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.26" />


<title>Notes on the Negative binomial distribution - Arman Anwar -- Lost in Space</title>
<meta property="og:title" content="Notes on the Negative binomial distribution - Arman Anwar -- Lost in Space">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Arman Anwar">
  </a>

  <ul class="nav-links">
    
    <li><a href="/">About</a></li>
    
    <li><a href="https://github.com/armaninspace">GitHub</a></li>
    
    <li><a href="https://www.linkedin.com/in/arman-anwar-a107961/">LinkedIn</a></li>
    
    <li><a href="/post">RandomStuff</a></li>
    
    <li><a href="https://twitter.com/armaninspace">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">2 min read</span>
    

    <h1 class="article-title">Notes on the Negative binomial distribution</h1>

    

    <div class="article-content">
      <p>A generalization of a geometric distribution. Negative binomial distribution describes the number of successes in a sequence of independent experiments performed until the <span class="math inline">\(r-\)</span>th failure. Formally, let <span class="math inline">\(\xi_1,\xi_2,\ldots\)</span> are results of experiments (that is all <span class="math inline">\(\xi\)</span> are independent and have Bernoulli distribution). <span class="math inline">\(S_n=\xi_1+\ldots+\xi_n\)</span> is the cumulative sum of <span class="math inline">\(\xi&#39;\)</span>s that represents the number of successes. The fact that there were <span class="math inline">\(r\)</span> failures can be written as <span class="math inline">\(S_n=n-r.\)</span> So, the negative binomial random variables is <span class="math display">\[
X=S_\tau, \tau=\min\{n\geq 1:S_n=n-r\}
\]</span> (here <span class="math inline">\(\tau\)</span> is the number of the last experiment, when <span class="math inline">\(r\)</span>-th failure occured).</p>
<p>Parameters: <span class="math inline">\(r\)</span> – number of failures after which we stop performing experiments, <span class="math inline">\(p\)</span> – probability of a success in a single experiment.</p>
<p>Values: <span class="math inline">\(\{0,1,2,\ldots\}.\)</span></p>
<p>Probability mass function: <span class="math display">\[
P(X=k)={k+r-1\choose k}p^{k}(1-p)^r, \ k\geq 0.
\]</span></p>
<div id="derivation" class="section level3">
<h3>Derivation</h3>
<p>The event <span class="math inline">\(X=k\)</span> means that when the <span class="math inline">\(r-\)</span>th failure occurd there were exactly <span class="math inline">\(k\)</span> successes. In particular it means that the process stopeed at <span class="math inline">\((k+r)\)</span>-th experiment: <span class="math display">\[
P(X=k)=P(\tau=k+r, S_{k+r}=k)=
\]</span> the last experiment is a failure <span class="math display">\[
=P(\xi_{k+r}=0, S_{k+r-1}=k)=
\]</span> by independence <span class="math display">\[
=(1-p)P(S_{k+r-1}=k)=
\]</span> the sum has a binomial distribution <span class="math display">\[
=(1-p){k+r-1\choose k}p^{k}(1-p)^(r-1)={k+r-1\choose k}p^{k}(1-p)^r.
\]</span></p>
<p>Moment generating function: <span class="math display">\[
M(t)=\frac{(1-p)^r}{(1-pe^t)^r}, \ t&lt;
\ln\frac{1}{p}
\]</span></p>
</div>
<div id="proof" class="section level3">
<h3>Proof</h3>
<p><span class="math display">\[
M(t)=Ee^{tX}=
\]</span> using probability mass function <span class="math display">\[
=\sum^\infty_{k=0} e^{tk}{k+r-1\choose k}p^{k}(1-p)^r=(1-p)^r\sum^\infty_{k=0} {k+r-1\choose k}(pe^t)^{k}=
\]</span> the sum is a Taylor expansion of <span class="math inline">\(\frac{1}{(1-x)^r}\)</span> around <span class="math inline">\(0,\)</span> evaluated at <span class="math inline">\(x=pe^t\)</span> (condition on <span class="math inline">\(t\)</span> ensures convergence) <span class="math display">\[
=\frac{(1-p)^r}{(1-pe^t)^r}
\]</span></p>
<p>Expectation: <span class="math inline">\(EX=\frac{pr}{1-p}\)</span></p>
<p>Variance: <span class="math inline">\(V(X)=\frac{pr}{(1-p)^2}\)</span></p>
</div>
<div id="derivation-1" class="section level3">
<h3>Derivation</h3>
<p>Expectation is the first derivative <span class="math inline">\(M&#39;(0).\)</span> We have <span class="math display">\[
M&#39;(t)=r\frac{p(1-p)^re^t}{(1-pe^t)^{r+1}}
\]</span> <span class="math display">\[
EX=M&#39;(0)=\frac{pr}{1-p}
\]</span> Second moment is the second derivative <span class="math inline">\(M&#39;&#39;(0).\)</span> We have <span class="math display">\[
M&#39;&#39;(t)=r\frac{p(1-p)^re^t}{(1-pe^t)^{r+1}}+r(r+1)\frac{p^2(1-p)^re^{2t}}{(1-pe^t)^{r+2}}
\]</span> <span class="math display">\[
EX^2=\frac{pr}{1-p}+\frac{r(r+1)p^2}{(1-p)^2}
\]</span> Variance is <span class="math display">\[
V(X)=\frac{pr}{1-p}+\frac{rp^2}{(1-p)^2}=\frac{pr}{(1-p)^2}
\]</span></p>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            &copy; Arman Anwar 2000 -- Today
          </li>
          
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

