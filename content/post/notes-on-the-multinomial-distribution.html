---
title: Notes on the Multinomial distribution
slug: notes-on-the-multinomial-distribution
categories: [Multinomial, Statistics, Probability, Elementary]
tags: []
---



<p>A multidimensional generalization of a binomial distribution. Assume that in each experiment there are <span class="math inline">\(k\)</span> possible outcomes (enumerated by <span class="math inline">\(1,2,\ldots,k\)</span>). Probabilities of these outcomes are <span class="math inline">\(p_1,p_2,\ldots,p_k,\)</span> so that <span class="math display">\[
p_1,\ldots,p_k\geq 0, \  p_1+\ldots+p_k=1.
\]</span> Multinomial distribution describeds the number of outcomes of each type in <span class="math inline">\(n\)</span> independent repetitions of the experiment:</p>
<p><span class="math inline">\((X_1,\ldots,X_k)=(m_1,\ldots,m_k)\)</span> if exactly <span class="math inline">\(m_1\)</span> experiments resulted in the outcome <span class="math inline">\(1,\)</span> exactly <span class="math inline">\(m_2\)</span> experiments resulted in the outcome <span class="math inline">\(2,\ldots,\)</span> exactly <span class="math inline">\(m_k\)</span> experiments resulted in the outcome <span class="math inline">\(k.\)</span></p>
<p>Parameters: <span class="math inline">\(n\)</span> – number of experiments, <span class="math inline">\(k\)</span> – number of outcomes in each experiment, <span class="math inline">\((p_1,\ldots,p_k)\)</span> – probability distribution of an outcome in each experiment.</p>
<p>Values: all sequences <span class="math inline">\((m_1,\ldots,m_k)\)</span> of non-negative integers that sum up to <span class="math inline">\(n\)</span> (there are <span class="math inline">\({n\choose n+k-1}\)</span> such sequences).</p>
<p>Probability mass function: <span class="math display">\[
P(X_1=m_1,\ldots,X_k=m_k)=\frac{n!}{m_1!\ldots m_k!}p^{m_1}_{1}\ldots p^{m_k}_k.
\]</span></p>
<p>{} Let <span class="math inline">\(\xi_l,\)</span> <span class="math inline">\(l=1,2,\ldots,n,\)</span> be the result of <span class="math inline">\(l\)</span>-th experiment, <span class="math inline">\(\xi_l\in\{1,2,\ldots,k\}.\)</span> The event <span class="math display">\[
\{X_1=m_1,\ldots,X_k=m_k\}
\]</span> means that exactly <span class="math inline">\(m_1\)</span> variables <span class="math inline">\(\xi_l=1,\)</span> exactly <span class="math inline">\(m_2\)</span> variables <span class="math inline">\(\xi_l=2,\)</span> <span class="math inline">\(\ldots,\)</span> exactly <span class="math inline">\(m_k\)</span> variables <span class="math inline">\(\xi_l=k.\)</span> When variables <span class="math inline">\(\xi_1,\ldots,\xi_l\)</span> are already grouped according to their values, the probability becomes <span class="math inline">\(p^{m_1}_1\ldots p^{m_k}_k.\)</span> The number of partitions of <span class="math inline">\(n\)</span> elements into <span class="math inline">\(k\)</span> groups by <span class="math inline">\(m_1,m_2,\ldots,m_k\)</span> elements is <span class="math inline">\(\frac{n!}{m_1!\ldots m_k!}\)</span>.</p>
<p>Moment generating function: <span class="math display">\[
M(t_1,\ldots,t_k)=Ee^{t_1X_1+\ldots+t_kX_k}=(p_1e^{t_1}+\ldots+p_k e^{t_k})^n
\]</span></p>
<p>{} <span class="math display">\[
M(t_1,\ldots,t_k)=Ee^{t_1X_1+\ldots+t_kX_k}=
\]</span> using probability mass function <span class="math display">\[
=\sum_{m_1+\ldots+m_k=n}\frac{n!}{m_1!\ldots m_k!}p^{m_1}_1\ldots p^{m_k}_k e^{t_1m_1+\ldots +t_km_k}=
\]</span> <span class="math display">\[
=\sum_{m_1+\ldots+m_k=n}\frac{n!}{m_1!\ldots m_k!}(p_1e^{t_1})^{m_1}\ldots (p_ke^{t_k})^{m_k}=
\]</span> by multinomial formula <span class="math display">\[
=(p_1 e^{t_1}+\ldots+p_k e^{t_k})^n
\]</span></p>
<p>Expectation: <span class="math inline">\(EX_j=np_j,\)</span> <span class="math inline">\(1\leq j\leq k.\)</span></p>
<p>Variance: <span class="math inline">\(V(X_j)=np_j(1-p_j),\)</span> <span class="math inline">\(1\leq j\leq k\)</span></p>
<p>Covariance: <span class="math inline">\(cov(X_i,X_j)=-np_ip_j,\)</span> <span class="math inline">\(1\leq i&lt;j\leq k.\)</span></p>
<p>{} The moment generating function of a single variable <span class="math inline">\(X_j\)</span> is obtained from <span class="math inline">\(M(t_1,\ldots,t_k)\)</span> by letting <span class="math inline">\(t_i=0,\)</span> <span class="math inline">\(i\ne j.\)</span> That is <span class="math display">\[
Ee^{t_jX_j}=M(0,\ldots,0,t_j,0,\ldots,0)=(p_1+\ldots+p_{j-1}+p_j e^{t_j}+p_{j+1}+\ldots+p_k)^n=
\]</span> using that <span class="math inline">\(p_1+\ldots+p_k=1\)</span> <span class="math display">\[
=(1-p_j+p_je^{t_j})^n
\]</span> This is exactly the moment generating function of a binomial distribution with parameters <span class="math inline">\(n,p_j:\)</span> <span class="math display">\[
X_j\sim Binomial(n,p_j)
\]</span> In particular <span class="math display">\[
EX_j=np_j, \ V(X_j)=np_j(1-p_j).
\]</span> To compute expectation of the product <span class="math inline">\(EX_iX_j,\)</span> <span class="math inline">\(i\ne j,\)</span> we take second mixed derivative of <span class="math inline">\(M\)</span> at point zero: <span class="math display">\[
\frac{\partial M}{\partial t_i}=np_ie^{t_i}(p_1 e^{t_1}+\ldots+p_k e^{t_k})^n
\]</span> <span class="math display">\[
\frac{\partial^2 M}{\partial t_i\partial t_j}=n(n-1)p_ip_je^{t_i+t_j}(p_1 e^{t_1}+\ldots+p_k e^{t_k})^{n-2}
\]</span> Put <span class="math inline">\(t_1=\ldots=t_k=0\)</span> and get <span class="math display">\[
E X_iX_j=n(n-1)p_ip_j
\]</span> So, the covariance <span class="math display">\[
cov(X_i,X_j)=EX_iX_j-EX_iEX_j=
\]</span> <span class="math display">\[
=n(n-1)p_ip_j-n^2p_ip_j=-np_ip_j.
\]</span></p>
