---
title: Notes on the Negative binomial distribution
slug: notes-on-the-negative-binomial-distribution
categories: [Negative-binomial, Statistics, Probability, Elementary]
tags: []
---



<p>A generalization of a geometric distribution. Negative binomial distribution describes the number of successes in a sequence of independent experiments performed until the <span class="math inline">\(r-\)</span>th failure. Formally, let <span class="math inline">\(\xi_1,\xi_2,\ldots\)</span> are results of experiments (that is all <span class="math inline">\(\xi\)</span> are independent and have Bernoulli distribution). <span class="math inline">\(S_n=\xi_1+\ldots+\xi_n\)</span> is the cumulative sum of <span class="math inline">\(\xi&#39;\)</span>s that represents the number of successes. The fact that there were <span class="math inline">\(r\)</span> failures can be written as <span class="math inline">\(S_n=n-r.\)</span> So, the negative binomial random variables is <span class="math display">\[
X=S_\tau, \tau=\min\{n\geq 1:S_n=n-r\}
\]</span> (here <span class="math inline">\(\tau\)</span> is the number of the last experiment, when <span class="math inline">\(r\)</span>-th failure occured).</p>
<p>Parameters: <span class="math inline">\(r\)</span> – number of failures after which we stop performing experiments, <span class="math inline">\(p\)</span> – probability of a success in a single experiment.</p>
<p>Values: <span class="math inline">\(\{0,1,2,\ldots\}.\)</span></p>
<p>Probability mass function: <span class="math display">\[
P(X=k)={k+r-1\choose k}p^{k}(1-p)^r, \ k\geq 0.
\]</span></p>
<div id="derivation" class="section level3">
<h3>Derivation</h3>
<p>The event <span class="math inline">\(X=k\)</span> means that when the <span class="math inline">\(r-\)</span>th failure occurd there were exactly <span class="math inline">\(k\)</span> successes. In particular it means that the process stopeed at <span class="math inline">\((k+r)\)</span>-th experiment: <span class="math display">\[
P(X=k)=P(\tau=k+r, S_{k+r}=k)=
\]</span> the last experiment is a failure <span class="math display">\[
=P(\xi_{k+r}=0, S_{k+r-1}=k)=
\]</span> by independence <span class="math display">\[
=(1-p)P(S_{k+r-1}=k)=
\]</span> the sum has a binomial distribution <span class="math display">\[
=(1-p){k+r-1\choose k}p^{k}(1-p)^(r-1)={k+r-1\choose k}p^{k}(1-p)^r.
\]</span></p>
<p>Moment generating function: <span class="math display">\[
M(t)=\frac{(1-p)^r}{(1-pe^t)^r}, \ t&lt;
\ln\frac{1}{p}
\]</span></p>
</div>
<div id="proof" class="section level3">
<h3>Proof</h3>
<p><span class="math display">\[
M(t)=Ee^{tX}=
\]</span> using probability mass function <span class="math display">\[
=\sum^\infty_{k=0} e^{tk}{k+r-1\choose k}p^{k}(1-p)^r=(1-p)^r\sum^\infty_{k=0} {k+r-1\choose k}(pe^t)^{k}=
\]</span> the sum is a Taylor expansion of <span class="math inline">\(\frac{1}{(1-x)^r}\)</span> around <span class="math inline">\(0,\)</span> evaluated at <span class="math inline">\(x=pe^t\)</span> (condition on <span class="math inline">\(t\)</span> ensures convergence) <span class="math display">\[
=\frac{(1-p)^r}{(1-pe^t)^r}
\]</span></p>
<p>Expectation: <span class="math inline">\(EX=\frac{pr}{1-p}\)</span></p>
<p>Variance: <span class="math inline">\(V(X)=\frac{pr}{(1-p)^2}\)</span></p>
</div>
<div id="derivation-1" class="section level3">
<h3>Derivation</h3>
<p>Expectation is the first derivative <span class="math inline">\(M&#39;(0).\)</span> We have <span class="math display">\[
M&#39;(t)=r\frac{p(1-p)^re^t}{(1-pe^t)^{r+1}}
\]</span> <span class="math display">\[
EX=M&#39;(0)=\frac{pr}{1-p}
\]</span> Second moment is the second derivative <span class="math inline">\(M&#39;&#39;(0).\)</span> We have <span class="math display">\[
M&#39;&#39;(t)=r\frac{p(1-p)^re^t}{(1-pe^t)^{r+1}}+r(r+1)\frac{p^2(1-p)^re^{2t}}{(1-pe^t)^{r+2}}
\]</span> <span class="math display">\[
EX^2=\frac{pr}{1-p}+\frac{r(r+1)p^2}{(1-p)^2}
\]</span> Variance is <span class="math display">\[
V(X)=\frac{pr}{1-p}+\frac{rp^2}{(1-p)^2}=\frac{pr}{(1-p)^2}
\]</span></p>
</div>
