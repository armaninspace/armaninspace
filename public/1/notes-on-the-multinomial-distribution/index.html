<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.26" />


<title>Notes on the Multinomial distribution - Arman Anwar -- Lost in Space</title>
<meta property="og:title" content="Notes on the Multinomial distribution - Arman Anwar -- Lost in Space">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/armaninspace">GitHub</a></li>
    
    <li><a href="https://www.linkedin.com/in/arman-anwar-a107961/">LinkedIn</a></li>
    
    <li><a href="https://twitter.com/armaninspace">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">2 min read</span>
    

    <h1 class="article-title">Notes on the Multinomial distribution</h1>

    
    <span class="article-date">0001/01/01</span>
    

    <div class="article-content">
      <p>A multidimensional generalization of a binomial distribution. Assume that in each experiment there are <span class="math inline">\(k\)</span> possible outcomes (enumerated by <span class="math inline">\(1,2,\ldots,k\)</span>). Probabilities of these outcomes are <span class="math inline">\(p_1,p_2,\ldots,p_k,\)</span> so that <span class="math display">\[
p_1,\ldots,p_k\geq 0, \  p_1+\ldots+p_k=1.
\]</span> Multinomial distribution describeds the number of outcomes of each type in <span class="math inline">\(n\)</span> independent repetitions of the experiment:</p>
<p><span class="math inline">\((X_1,\ldots,X_k)=(m_1,\ldots,m_k)\)</span> if exactly <span class="math inline">\(m_1\)</span> experiments resulted in the outcome <span class="math inline">\(1,\)</span> exactly <span class="math inline">\(m_2\)</span> experiments resulted in the outcome <span class="math inline">\(2,\ldots,\)</span> exactly <span class="math inline">\(m_k\)</span> experiments resulted in the outcome <span class="math inline">\(k.\)</span></p>
<p>Parameters: <span class="math inline">\(n\)</span> – number of experiments, <span class="math inline">\(k\)</span> – number of outcomes in each experiment, <span class="math inline">\((p_1,\ldots,p_k)\)</span> – probability distribution of an outcome in each experiment.</p>
<p>Values: all sequences <span class="math inline">\((m_1,\ldots,m_k)\)</span> of non-negative integers that sum up to <span class="math inline">\(n\)</span> (there are <span class="math inline">\({n\choose n+k-1}\)</span> such sequences).</p>
<p>Probability mass function: <span class="math display">\[
P(X_1=m_1,\ldots,X_k=m_k)=\frac{n!}{m_1!\ldots m_k!}p^{m_1}_{1}\ldots p^{m_k}_k.
\]</span></p>
<p>{} Let <span class="math inline">\(\xi_l,\)</span> <span class="math inline">\(l=1,2,\ldots,n,\)</span> be the result of <span class="math inline">\(l\)</span>-th experiment, <span class="math inline">\(\xi_l\in\{1,2,\ldots,k\}.\)</span> The event <span class="math display">\[
\{X_1=m_1,\ldots,X_k=m_k\}
\]</span> means that exactly <span class="math inline">\(m_1\)</span> variables <span class="math inline">\(\xi_l=1,\)</span> exactly <span class="math inline">\(m_2\)</span> variables <span class="math inline">\(\xi_l=2,\)</span> <span class="math inline">\(\ldots,\)</span> exactly <span class="math inline">\(m_k\)</span> variables <span class="math inline">\(\xi_l=k.\)</span> When variables <span class="math inline">\(\xi_1,\ldots,\xi_l\)</span> are already grouped according to their values, the probability becomes <span class="math inline">\(p^{m_1}_1\ldots p^{m_k}_k.\)</span> The number of partitions of <span class="math inline">\(n\)</span> elements into <span class="math inline">\(k\)</span> groups by <span class="math inline">\(m_1,m_2,\ldots,m_k\)</span> elements is <span class="math inline">\(\frac{n!}{m_1!\ldots m_k!}\)</span>.</p>
<p>Moment generating function: <span class="math display">\[
M(t_1,\ldots,t_k)=Ee^{t_1X_1+\ldots+t_kX_k}=(p_1e^{t_1}+\ldots+p_k e^{t_k})^n
\]</span></p>
<p>{} <span class="math display">\[
M(t_1,\ldots,t_k)=Ee^{t_1X_1+\ldots+t_kX_k}=
\]</span> using probability mass function <span class="math display">\[
=\sum_{m_1+\ldots+m_k=n}\frac{n!}{m_1!\ldots m_k!}p^{m_1}_1\ldots p^{m_k}_k e^{t_1m_1+\ldots +t_km_k}=
\]</span> <span class="math display">\[
=\sum_{m_1+\ldots+m_k=n}\frac{n!}{m_1!\ldots m_k!}(p_1e^{t_1})^{m_1}\ldots (p_ke^{t_k})^{m_k}=
\]</span> by multinomial formula <span class="math display">\[
=(p_1 e^{t_1}+\ldots+p_k e^{t_k})^n
\]</span></p>
<p>Expectation: <span class="math inline">\(EX_j=np_j,\)</span> <span class="math inline">\(1\leq j\leq k.\)</span></p>
<p>Variance: <span class="math inline">\(V(X_j)=np_j(1-p_j),\)</span> <span class="math inline">\(1\leq j\leq k\)</span></p>
<p>Covariance: <span class="math inline">\(cov(X_i,X_j)=-np_ip_j,\)</span> <span class="math inline">\(1\leq i&lt;j\leq k.\)</span></p>
<p>{} The moment generating function of a single variable <span class="math inline">\(X_j\)</span> is obtained from <span class="math inline">\(M(t_1,\ldots,t_k)\)</span> by letting <span class="math inline">\(t_i=0,\)</span> <span class="math inline">\(i\ne j.\)</span> That is <span class="math display">\[
Ee^{t_jX_j}=M(0,\ldots,0,t_j,0,\ldots,0)=(p_1+\ldots+p_{j-1}+p_j e^{t_j}+p_{j+1}+\ldots+p_k)^n=
\]</span> using that <span class="math inline">\(p_1+\ldots+p_k=1\)</span> <span class="math display">\[
=(1-p_j+p_je^{t_j})^n
\]</span> This is exactly the moment generating function of a binomial distribution with parameters <span class="math inline">\(n,p_j:\)</span> <span class="math display">\[
X_j\sim Binomial(n,p_j)
\]</span> In particular <span class="math display">\[
EX_j=np_j, \ V(X_j)=np_j(1-p_j).
\]</span> To compute expectation of the product <span class="math inline">\(EX_iX_j,\)</span> <span class="math inline">\(i\ne j,\)</span> we take second mixed derivative of <span class="math inline">\(M\)</span> at point zero: <span class="math display">\[
\frac{\partial M}{\partial t_i}=np_ie^{t_i}(p_1 e^{t_1}+\ldots+p_k e^{t_k})^n
\]</span> <span class="math display">\[
\frac{\partial^2 M}{\partial t_i\partial t_j}=n(n-1)p_ip_je^{t_i+t_j}(p_1 e^{t_1}+\ldots+p_k e^{t_k})^{n-2}
\]</span> Put <span class="math inline">\(t_1=\ldots=t_k=0\)</span> and get <span class="math display">\[
E X_iX_j=n(n-1)p_ip_j
\]</span> So, the covariance <span class="math display">\[
cov(X_i,X_j)=EX_iX_j-EX_iEX_j=
\]</span> <span class="math display">\[
=n(n-1)p_ip_j-n^2p_ip_j=-np_ip_j.
\]</span></p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

